% README
%
% Author       : Frank E. Curtis
% Description  : README file.

% Please cite:
%   F. E. Curtis and M. L. Overton, ``A Sequential Quadratic Programming
%   Algorithm for Nonconvex, Nonsmooth Constrained Optimization,''
%   in third round of review for SIAM Journal on Optimization.

% This code runs a sequential quadratic programming method with gradient
% sampling sampling (SQP-GS) or a sequential linear programming method with
% gradient sampling (SLP-GS) for nonconvex, nonsmooth constrained
% optimization.  The problem of interest is formulated as
%   minimize f(x) subject to cE(x) = 0, cI(x) <= 0,
% where f, cE, and cI are all continuously differentiable on open dense
% subsets of R^n.  Global convergence results hold with probability one if
% the problem functions are also locally Lipschitz on R^n; see the above
% citation.

% A sample problem can be run, if the Matlab Optimization Toolbox is
% installed, via the commands
%   >> S = SLQPGS('sample');
%   >> S.optimize;
%   >> S.getSolution;
% An output file, slqpgs.out, will be generated in the current directory.

% Please note that the algorithm is stochastic in nature, meaning that two
% runs for the same problem instance can yield different results!  The
% states of Matlab's built-in rand and randn functions should be set prior
% to a run to obtain reproducable results.

% A user need only create a slqpgs_inputs.m file and another .m file for
% the data function evaluations to run the code.  Please refer to the
% sample problem in ./sample to see how these files should be formatted and
% see below for more details.  If a QP or LP solver other than Matlab's
% built-in quadprog or linprog, respectively, is desired, then changes to
% the solveSubproblem method in the Direction class may be necessary.  We
% recommend the solver MOSEK (www.mosek.com).  For that solver one need
% only set the path to include the directory containing MOSEK's quadprog
% function; no changes to the solveSubproblem method are necessary as the
% formatting for MOSEK's quadprog is the same as that for Matlab's.

% The remainder of this file includes more detail about how to create a
% problem instance, how to read the output file, and how to change the
% input parameters.

% CREATING A PROBLEM INSTANCE

% Please see the sample problem in ./sample as a guide.  The required
% fields for the input structure i, returned by slqpgs_inputs.m, are:
%   i.nV ~ number of variables
%   i.nE ~ number of equality constraints
%   i.nI ~ number of inequality constraints
%   i.pO ~ number of sample points for the objective
%   i.pE ~ if i.nE > 0, vector of length i.nE indicating numbers of sample
%           points for each equality constraint
%   i.pI ~ if i.nI > 0, vector of length i.nI indicating numbers of sample
%           points for each inequality constraint
%   i.f  ~ handle for file evaluating the problem functions and gradients
%   i.x  ~ initial point, a column vector of length i.nV
% Optional fields for the input structure i, returned by slqpgs_inputs, are:
%   i.algorithm  ~ algorithm option, 0 = SQP-GS and 1 = SLP-GS; default is 0
%   i.sp_problem ~ subproblem option, 0 = primal and 1 = dual; default is 0
%   i.sp_solver  ~ subproblem solver handle; default is 'quadprog'
%   i.sp_options ~ subproblem solver options; default is optimset('Display','off')
%   i.stat_tol   ~ termination tolerance; default is 1e-06
%   i.feas_tol   ~ feasibility tolerance; default is 1e-04
%   i.iter_max   ~ maximum iterations; default is 1e+03
%   i.d          ~ data structure to be passed to problem function evaluator

% The numbers of sample points --- i.pO, i.pE, and i.pI --- should be
% chosen based on the smoothness of each problem function.  If a given
% function is smooth, then the number of sample points for that function
% may be set to zero.  If a given function is nonsmooth, then the number of
% sample points should be positive.  The convergence theory in the paper
% cited above requires that at least i.nV+1 points be sampled for each
% nonsmooth function.  However, if a given function depends on only m < i.nV
% variables, then it is reasonable to set the number of sample points to m+1
% (if not more).  Generally speaking, more sample points will produce
% better search directions, but at a higher cost per iteration.

% The .m file for evaluating the problem function and gradient values
% should follow the format in the example; i.e., it should contain a switch
% (or if statements) on the input o and return the desired function or
% gradient value through the output v.  Note that the objective gradient is
% expected to be a column vector and any constraint gradient is expected to
% be a row vector.  Also note that constraint function and gradient values
% are to be returned individually according to the index j provided as an
% input.  This is in contrast to many optimization codes that return an
% entire Jacobian matrix at once.

% READING THE OUTPUT FILE AND THE RETURNED VALUES

% The problem size is printed before the output of the optimization
% routine.  It is advisable to verify that these sizes match the problem
% one is intending to solve.  The subproblem size is that for the QP or LP
% solve to compute the search direction during every iteration.  This size
% will change depending on if the primal or dual form of the subproblem is
% solved (see i.sp_problem in the list of inputs above).

% A short description of each item in the output file is as follows.
% Notation in parentheses refers to quantities as they are denoted in the
% paper cited above.
%   Iter.      ~ iteration number (k)
%   Objective  ~ objective value (f)
%   Infeas.    ~ infeasibility measure (v)
%   Pen. Par.  ~ penalty parameter value (rho)
%   Merit      ~ merit/penalty function value (phi)
%   Msg.       ~ subproblem solver message (see below)
%   ||Step||   ~ norm of computed search direction (||d||)
%   Mod. Red.  ~ reduction in model of penalty function for computed search
%                direction (Delta-q_rho)
%   Samp. Rad. ~ sampling radius (epsilon)
%   Inf. Tol.  ~ infeasibility tolerance (theta)
%   Stepsize   ~ steplength determined by line search (alpha)
% Note that the subproblem solver message should be observed to ensure that
% the subproblems are being solved sufficiently accurately to produce good
% steps for the nonlinear optimization.  For example, Matlab's quadprog
% returns a value of 1 for a successful solve.

% Finally, the summary results at the end of the output should be
% self-explanatory.  The only exception is the number of gradient
% evaluations reported.  The number of gradient evaluations is *not*
% necessarily the number of times the gradient of a given problem function
% is evaluated.  Rather, it is the number of times that the "gradient
% evaluator" is called, which may then internally evaluate a given
% function's gradient a number of times depending on the number of sample
% points specified for that function.  Overall, if one desires to know the
% number of times that, say, the objective's gradient is evaluated, then
% take the number of "Function Evaluations" and add to that the number of
% sample points for the objective times the number of "Iterations".

% CHANGING INPUT PARAMETERS

% There are a variety of parameters whose value may affect the performance
% of the code on a given problem instance.  Besides those listed above as
% inputs to the code (e.g., the initial point i.x), there are constants
% defined in the Parameters class that may be changed.  Expert users may
% change these values, though it is not recommended.
